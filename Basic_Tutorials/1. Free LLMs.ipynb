{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open LLM Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "if load_dotenv(\"../.env\"):\n",
    "    GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "    HF_API_TOKEN=os.getenv('HF_API_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is [OLlama](https://ollama.com/) ?\n",
    "Ollama allows you run LLMs locally!\n",
    "\n",
    "Github: https://github.com/ollama/ollama and https://github.com/ollama/ollama-python\n",
    "\n",
    "## Instructions to Run\n",
    "\n",
    "1. Install docker if you donÂ´t have it installed yet: `bash docker.sh`\n",
    "2. Start docker service: `sudo service docker start`\n",
    "3. Pull Ollama: `sudo docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama`\n",
    "4. Optionally you can talk directly with the model:  \n",
    "   a) How to run a Model: `sudo docker exec -it ollama ollama run llama3`\n",
    "\n",
    "After everything is properly downloaded and running, run the Jupyter cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3',\n",
       " 'created_at': '2024-04-27T11:46:42.190464664Z',\n",
       " 'message': {'role': 'assistant', 'content': 'Albert Einstein.'},\n",
       " 'done': True,\n",
       " 'total_duration': 4512020759,\n",
       " 'load_duration': 3823014,\n",
       " 'prompt_eval_count': 28,\n",
       " 'prompt_eval_duration': 3383385000,\n",
       " 'eval_count': 4,\n",
       " 'eval_duration': 980167000}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama.chat(model='llama3', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Who do you think is most famous physisist in the world from all times ? Give me a simple and direct answer',\n",
    "  },\n",
    "])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an AI, not a human."
     ]
    }
   ],
   "source": [
    "#Streaming responses\n",
    "stream = ollama.chat(\n",
    "    model='llama3',\n",
    "    messages=[{'role': 'user', 'content': 'Who old are you? Simple and direct answer only'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatWithLocalOllama(text:str):\n",
    "    return ollama.generate(model=\"llama3\", prompt=text, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Earth, the next planet in our solar system is Mars."
     ]
    }
   ],
   "source": [
    "question = \"What planet comes after Earth ? Direct answer please\"\n",
    "for chunk in chatWithLocalOllama(question):\n",
    "    print(chunk['response'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What if Ollama is hosted somewhere else ? You have to create custom client\n",
    "from ollama import Client\n",
    "\n",
    "def chatWithRemoteOllama(text:str):\n",
    "    client = Client(host='http://localhost:11434')\n",
    "    return client.generate(model=\"llama3\", prompt=text, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A famous painting by Leonardo da Vinci, featuring a woman with an enigmatic smile."
     ]
    }
   ],
   "source": [
    "question = \"What is the Mona Lisa? Answer in the shortest way possible\"\n",
    "for chunk in chatWithRemoteOllama(question):\n",
    "    print(chunk['response'], end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Bowser!\"\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "2nd iteration\n",
      "\n",
      "\"Koopa King Bowser! That no-good, fire-breathing turtle's at it again!\""
     ]
    }
   ],
   "source": [
    "async def chatWithRemoteOllamaAsync(text:str):\n",
    "    client = Client(host='http://localhost:11434')\n",
    "    return client.chat(model=\"llama3\", messages=text, stream=True)\n",
    "\n",
    "messages = [\n",
    "    {'role':'system','content':'You are Super Mario'},\n",
    "    {'role':'user', 'content':'Luigi has gonne missing. What is your first thought ? be direct and simple '}\n",
    "    ]\n",
    "\n",
    "response = ''\n",
    "for chunk in await chatWithRemoteOllamaAsync(messages):\n",
    "    response += chunk['message']['content']\n",
    "    print(chunk['message']['content'], end='', flush=True)\n",
    "    \n",
    "messages.append({'role':'assistant','content':response})\n",
    "\n",
    "print('\\n')\n",
    "print('-'*120)\n",
    "print('2nd iteration\\n')\n",
    "\n",
    "messages = [\n",
    "    *messages,  # Include the history from the first call\n",
    "    {'role': 'user', 'content': 'Who Kidnapped him?'}\n",
    "]\n",
    "\n",
    "for chunk in await chatWithRemoteOllamaAsync(messages):\n",
    "    print(chunk['message']['content'], end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: model 'does-not-yet-exist' not found, try pulling it first\n"
     ]
    }
   ],
   "source": [
    "#ERROR HANDLING! \n",
    "model = 'does-not-yet-exist'\n",
    "\n",
    "try:\n",
    "  ollama.chat(model)\n",
    "except ollama.ResponseError as e:\n",
    "  print('Error:', e.error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [HuggingFace Transformers](https://github.com/huggingface/transformers)\n",
    "\n",
    "Hugging Face Transformers is a state-of-the-art machine learning library that provides easy access to pre-trained models for various tasks across different modalities. Here are some key features:\n",
    "\n",
    "## Modalities Supported\n",
    "\n",
    "- **Natural Language Processing (NLP)**: Tasks include text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.\n",
    "- **Computer Vision**: Tasks include image classification, object detection, and segmentation.\n",
    "- **Audio**: Tasks include automatic speech recognition and audio classification.\n",
    "- **Multimodal**: Tasks include table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.\n",
    "\n",
    "## Pretrained Models\n",
    "\n",
    "Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch.\n",
    "\n",
    "[100 projects using Transformers](https://github.com/huggingface/transformers/blob/main/awesome-transformers.md)\n",
    "\n",
    "### Possible Tasks using Pipelines\n",
    "\n",
    "- \"text-generation\"\n",
    "- \"conversational\"\n",
    "- \"document-question-answering\"\n",
    "- \"translation\"\n",
    "- \"image-to-text\"\n",
    "- \"text-to-audio\" (alias \"text-to-speech\" available)\n",
    "- \"audio-classification\"`\n",
    "- \"automatic-speech-recognition\"\n",
    "- \"depth-estimation\"\n",
    "- \"feature-extraction\"\n",
    "- \"fill-mask\"\n",
    "- \"image-classification\"\n",
    "- \"image-feature-extraction\"\n",
    "- \"image-segmentation\"\n",
    "- \"image-to-image\"\n",
    "- \"token-classification\" (alias \"ner\" available)\n",
    "- \"translation_xx_to_yy\"\n",
    "- \"video-classification\"\n",
    "- \"visual-question-answering\"\n",
    "- \"zero-shot-classification\"\n",
    "- \"zero-shot-image-classification\"\n",
    "-\"zero-shot-audio-classification\"\n",
    "- \"zero-shot-object-detection\"\n",
    "\n",
    "Hugging-Face main homepage: https://huggingface.co/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998741149902344}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment = pipeline(task=\"sentiment-analysis\", token=HF_API_TOKEN, device=\"cpu\")\n",
    "results = sentiment(\"What a lovely day\")\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/wav2vec2-base-960h and revision 55bb623 (https://huggingface.co/facebook/wav2vec2-base-960h).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'IS TIONTESCIGI HECUIESIMIANTE GUVOS BY THE SABIRSIO PIPLYIN AD GET UP TO I TO THE HAYUN FACE A MUTOBO BY THE TRUSCREVIRTESTUS'},\n",
       " {'text': 'THIS MODO CAN ONLY UNDERSTAND ENGLISH'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "transcriber = pipeline(task=\"automatic-speech-recognition\", token=HF_API_TOKEN, device=\"cpu\")\n",
    "transcriber([\"./audio1.ogg\", \"./audio2_en.ogg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': ' Este Ã© um teste de reconhecimento de voz para saber se o pipeline gratuito da Rallyon Face Ã© muito bom para transcrever textos.'},\n",
       " {'text': ' This model can only understand English.'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "transcriber = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-tiny\",token=HF_API_TOKEN, device=\"cpu\")\n",
    "transcriber([\"./audio1.ogg\", \"./audio2_en.ogg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'leopard, Panthera pardus', 'score': 0.9643744230270386},\n",
       " {'label': 'jaguar, panther, Panthera onca, Felis onca',\n",
       "  'score': 0.03195194527506828},\n",
       " {'label': 'cheetah, chetah, Acinonyx jubatus',\n",
       "  'score': 0.0015117806615307927},\n",
       " {'label': 'snow leopard, ounce, Panthera uncia',\n",
       "  'score': 0.0007983926334418356},\n",
       " {'label': 'lion, king of beasts, Panthera leo',\n",
       "  'score': 0.00022117019398137927}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# yes the task is automatically infered from the model also\n",
    "vision_classifier = pipeline(\n",
    "    model=\"google/vit-base-patch16-224\", token=HF_API_TOKEN, device=\"cpu\")\n",
    "preds = vision_classifier(\n",
    "    images=\"https://th.bing.com/th/id/OIP.YjlrCGml5fb7B2pBqtdivQHaE7?rs=1&pid=ImgDetMain\"\n",
    ")\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "[{'score': 0.6036366820335388, 'answer': '$154.06', 'start': 75, 'end': 75}]\n",
      "[{'score': 0.9854755401611328, 'answer': 'Newset of pedal arms', 'start': 57, 'end': 60}]\n",
      "[{'score': 0.672887921333313, 'answer': 'us-001', 'start': 16, 'end': 16}]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "[{'score': 0.9999239444732666, 'answer': '26/02/2019', 'start': 42, 'end': 42}]\n",
      "[{'score': 0.9997648000717163, 'answer': 'East Repair Inc.', 'start': 1, 'end': 3}]\n",
      "[{'score': 0.9997523427009583, 'answer': 'John Smith', 'start': 17, 'end': 18}]\n",
      "[{'score': 0.2696015536785126, 'answer': 'John Smith', 'start': 17, 'end': 18}]\n",
      "[{'score': 0.751903772354126, 'answer': 'John Smith', 'start': 17, 'end': 18}]\n"
     ]
    }
   ],
   "source": [
    "img=\"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\"\n",
    "initial_questions = [\n",
    "    \"What is the total price of the invoice ?\",\n",
    "    \"What is the 2nd item description ?\",\n",
    "    \"What is the invoice number ?\",\n",
    "]\n",
    "\n",
    "more_questions = [\n",
    "    \"What is the Due date?\",\n",
    "    \"What is the name of the company who created this invoice?\",\n",
    "    \"What is the name of the customer who paid this ?\",\n",
    "    \"What is the address of the recipient ?\",\n",
    "    \"What is the complete address under SHIP TO ?\"\n",
    "]\n",
    "\n",
    "image_feature_extraction = pipeline(model=\"impira/layoutlm-document-qa\")\n",
    "data = lambda i: {\"image\": img, \"question\": initial_questions[i]}\n",
    "print('-'*120)\n",
    "for i in range(3):\n",
    "    print(image_feature_extraction(data(i)))\n",
    "    \n",
    "# Imagine this is a very large dataset . With yield you defer the execution of the function until it is needed\n",
    "def large_data():\n",
    "    for i in range(5):\n",
    "        yield {\"image\": img, \"question\": more_questions[i]}\n",
    "\n",
    "print('-'*120)\n",
    "for out in image_feature_extraction(large_data()):\n",
    "    print(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"What are you trained for?\\n\\nI am a big fan of being a professional wrestler, though, and it's also fun having my own video game company where I don't need video game help. I have the whole world playing around with my\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "text_generation = pipeline(\"text-generation\", model=\"openai-community/gpt2\", token=HF_API_TOKEN, device=\"cpu\")\n",
    "question = \"What are you trained for?\"\n",
    "result = text_generation(question)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Groq](https://wow.groq.com/)\n",
    "\n",
    "## What is Groq ?  \n",
    "\n",
    "   Groq is an open-source, distributed, and scalable graph database that allows users to store and query complex relationships between data entities. It's designed to handle large-scale graph data and provides a flexible and efficient way to store and query graph data.\n",
    "\n",
    "## Is Groq free ?\n",
    "\n",
    "   Groq is open-source, which means it is free to use, modify, and distribute. However, it's worth noting that Groq is still an actively developing project, and while it's free to use, it may not have the same level of support or resources as a commercial product. Additionally, while Groq is free, it may require additional infrastructure and resources to set up and maintain, depending on the scale and complexity of your use case.\n",
    "\n",
    "\n",
    "try the Groq [playground](https://console.groq.com/playground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to help!\n",
      "\n",
      "1. Groq is an open-source, distributed, and scalable graph database that allows users to store and query complex relationships between data entities. It's designed to handle large-scale graph data and provides a flexible and efficient way to store and query graph data. Groq is built on top of the Apache Arrow and Apache Parquet data formats, making it compatible with a wide range of data sources and tools.\n",
      "\n",
      "2. Groq is open-source software, which means it is free to use, modify, and distribute. The project is maintained by a community of developers and contributors, and the source code is available on GitHub under the Apache 2.0 license. This means that users can use Groq without any licensing fees or restrictions, and they can also contribute to the project and help shape its development.\n",
      "\n",
      "It's worth noting that while Groq is free and open-source, it may require some technical expertise to set up and use, especially for complex graph queries. However, the community and documentation are available to help users get started and overcome any challenges they may encounter.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def chat(question):\n",
    "    llm = ChatGroq(temperature=0, groq_api_key=GROQ_API_KEY,\n",
    "                   model_name=\"Llama3-8b-8192\")\n",
    "    system = \"You are a helpful assistant.\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\"system\", system), (\"human\", \"{text}\")])\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"text\": question})\n",
    "    return response\n",
    "\n",
    "\n",
    "result = chat(\"1. What is Groq? 2.Is Groq Free ?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you're asking about Llama3! Llama3 is an open-source, cloud-based, and scalable machine learning platform developed by Google. It's designed to simplify the process of deploying and managing machine learning models in production environments.\n",
      "\n",
      "Llama3 provides a range of features, including:\n",
      "\n",
      "1. **Model serving**: Llama3 allows you to deploy and manage machine learning models in a scalable and efficient manner.\n",
      "2. **Model management**: You can manage your models, including versioning, monitoring, and updating them as needed.\n",
      "3. **Scalability**: Llama3 is designed to handle large volumes of traffic and scale to meet the needs of your application.\n",
      "4. **Security**: The platform provides robust security features to ensure the integrity and confidentiality of your data.\n",
      "\n",
      "Llama3 is particularly useful for organizations that rely heavily on machine learning models in their applications. It helps streamline the process of deploying and managing these models, making it easier to integrate them into your production environment.\n",
      "\n",
      "Would you like to know more about Llama3 or is there something else I can help you with?"
     ]
    }
   ],
   "source": [
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "async def async_chat(text):\n",
    "    llm = ChatGroq(temperature=0, groq_api_key=GROQ_API_KEY,\n",
    "                   model_name=\"Llama3-8b-8192\")\n",
    "    system = \"You are a helpful assistant.\"\n",
    "    human = \"{text}\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\"system\", system), (\"human\", human)])\n",
    "    chain = prompt | llm\n",
    "    return chain.stream({\"text\": text})\n",
    "\n",
    "\n",
    "for chunk in await async_chat({\"text\": \"\"\"\n",
    "                           What is Llama3 ? \n",
    "                           \"\"\"}):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was trained on a dataset that was current up to 2021. However, please note that my training data may not reflect any updates or changes that have occurred after that date. If you have any specific questions or topics you'd like to discuss, I'll do my best to provide you with accurate and helpful information."
     ]
    }
   ],
   "source": [
    "for chunk in await async_chat({\"text\":\"What are the latest date of your training data ?\"}):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
