{
    "id": "90cfd251-b076-4f2e-b637-bbf66a6265a2",
    "url": "https://youtu.be/7jMIsmwocpM?si=Ipw-gE8afVLFpZG7.youtube",
    "title": "6 Ways to Run ChatGPT Alternatives in Your Machine (Including Llama3)",
    "docAuthor": "Semaphore CI",
    "description": "Open-source AI and Large Language Models are getting better and better. If we can replace ChatGPT or Bard with them we would gain a great deal of privacy and use these models in cases we previously couldn't before (like when dealing with sensitive or proprietary data).\n\nIn this video, we'll learn 6 ways to run various open-source large language models locally.\n\nğŸ”— Useful links:\nI tried 7 ChatGPT alternatives: https://youtu.be/Fd_su8OamSE\nHow to train a model using CI/CD: https://youtu.be/6X8BqtmXoVo\nBlog post with examples: https://semaphoreci.com/blog/local-llm\nHuggingFace: https://huggingface.co\nLangChain: https://www.langchain.com\nLlama.cpp: https://github.com/ggerganov/llama.cpp\nLlamafile: https://github.com/Mozilla-Ocho/llamafile\nOllama: https://ollama.ai\nGPT4ALL: https://gpt4all.io/index.html\n\n\n================================================\nTimestamps:\n0:00 Intro\n0:54 Hardware requirements\n2:04 (1) How to use HuggingFace ğŸ¤— and Transformers ğŸ¤–\n8:35 (2) LangChain\n11:58 (3) Llama.cpp\n17:21 (4) Llamafile\n20:39 (5) Ollama.ai\n23:43 (6) GPT4ALL\n27:10 Conclusion\n================================================\n\n#llm #ai #localllm #llama2 #openai #chatgpt #nlp #machinelearning #ml #development #programming #devops #tutorial #llama3",
    "docSource": "https://youtu.be/7jMIsmwocpM?si=Ipw-gE8afVLFpZG7",
    "chunkSource": "link://https://youtu.be/7jMIsmwocpM?si=Ipw-gE8afVLFpZG7",
    "published": "5/13/2024, 6:41:01 PM",
    "wordCount": 3967,
    "pageContent": "large language models like chat GPT and AntrophicÂ \nClaude or Google bar are really powerful toolsÂ Â with lots of use cases however all these ServicesÂ \nhave the same big drawback privacy what happensÂ Â if we are dealing with sensitive data we canÂ \nsend that over the wire even uh for reasons ofÂ Â compliance or even for reasons of trust we can&#39;tÂ \nreally trust uh uh sending data over to wire andÂ Â we don&#39;t know what&#39;s going to happen to that soÂ \nthis is where having a a private language model weÂ Â can run in our own machines really valuable thereÂ \nare literally hundreds of open- source models thatÂ Â have already been trained and are ready to use soÂ \njoin me in my quest to find a chatGPT replacementÂ Â I can run locally in my laptop so before we startÂ \nlet&#39;s set the expectations there are hundreds ofÂ Â thousands of open- source models out there someÂ \ntrained by individuals on some like this trainÂ Â like corporation like meta to run some of theseÂ \nmodels we need a beefy machine with lots of memoryÂ Â and even a GPU some smaller models can run on aÂ \nlaptop but even with the Best Hardware open sourceÂ Â model are smaller in size and a little bit lessÂ \npowerful than polished products like chatgpt afterÂ Â all open has dedicated hundreds of Engineers toÂ \nmaintaining the thing in this leak document it&#39;sÂ Â called we have no mold and neither does open AiÂ \nand it goes into great detail into explaining howÂ Â open source has solved some problems like runningÂ \nlanguage models on phones or multimodality and uhÂ Â here we have a comparison between uh closed andÂ \nopen source models open source models are quicklyÂ Â Going over new iterations one or two weeks apartÂ \nand even they are smaller they can do a very goodÂ Â job compared to close Source models that areÂ \nmaybe a thousand times bigger so you probablyÂ Â heard about the site hugging face. this is theÂ \nlargest repository for open source models weÂ Â can find projects and models that are uploadedÂ \nby individuals or even by corporations you canÂ Â find model from Microsoft from meta and here umÂ \nwe have a lot of things this is a really busy hÂ Â page because it&#39;s not only about language modelÂ \nbut also about things like image generation youÂ Â know and and and really estate of the art uhÂ \nAI but what we want for for our use case forÂ Â language models is to go here to models and thenÂ \nwe have to filter uh on here on the left side byÂ Â tasks so we have here a a whole lot of task likeÂ \naudio or you know image processing what we wantÂ Â is something along natural language processingÂ \nand I hear this is the first thing we need to payÂ Â attention because the kind of work we are needingÂ \ndepends on what task we we choose here so it&#39;s notÂ Â the same to to select translation than to selectÂ \nsummarization ideally we want to pick the best uhÂ Â model for our job in my case for example I wouldÂ \nlike to have something to chat with so I will beÂ Â conversational and this starts to filter um theÂ \nother big filter here is in the library sectionÂ Â and this is all the kinds of libraries and waysÂ \nof running the language models uh most of theseÂ Â models are Transformers Transformers is a type ofÂ \nmodel uh this is the T in J GPT and it&#39;s also aÂ Â library from Hugging Face a python library thatÂ \nsimplifies setting up a model you know we alsoÂ Â have popular libraries like pytorch tensorflowÂ \nJSX which is a Google library then we have someÂ Â formats like GGUF which is a format used forÂ \nLkama models and Keras is another big LibraryÂ Â so we&#39;re going to pick something here that weÂ \nknow how to use or we want to learn how to useÂ Â uh for for starters let&#39;s pick Transformers andÂ \nthen we can sort here on the on the right sideÂ Â for trending or most downloads and open oneÂ \nof these models will will give us informationÂ Â about the model let&#39;s speak one that I know itÂ \nwill work in my machine because um it&#39;s quiteÂ Â Limited in memory and for example we can pick uhÂ \nMicrosoft dialog GPT this is a gpt2 model whichÂ Â is a a smaller and older model uh but here weÂ \nhave the model car which basically explains uhÂ Â how the model works and more importantly how toÂ \nuse it so you have some Snippets of code you canÂ Â copy and and paste on your on your computer andÂ \nrun the model if you don&#39;t find any informationÂ Â here that&#39;s useful you can always click here onÂ \nuse in Transformers and you will find a coupleÂ Â of Snippets normally the first snippet is theÂ \none that H starts uh uh your program and thisÂ Â picks this models uh the Transformers Library willÂ \ndownload the model automatically in your machineÂ Â and set it up and obviously you need to completeÂ \nthe code with um the the logic you want to do inÂ Â your in your library in your project so let&#39;s goÂ \nand pick one of these so to get started we areÂ Â going to need a few tools like the a compilerÂ \nmake or cake these are all things that I haveÂ Â installed in my machine but you probably needÂ \nto have installed so the the library is compileÂ Â we are going to start by installing a pytorch andÂ \ntensorflow which will cover most of the things weÂ Â are going to need for for this example we can useÂ \npip to install both libraries and then we&#39;re goingÂ Â to install Transformers sentence piece this is aÂ \nversion of Transformers that uh has the completeÂ Â package you know all the tools you will need toÂ \nrun hiking phase Transformers so let&#39;s create aÂ Â new file chatgpt and we are going to paste theÂ \ncode here this is the code in the model uh weÂ Â are using this model and this is the exact codeÂ \nthat we h we find in the model card uh the firstÂ Â time this rant is going to download the modelÂ \nwhich in this case is about 3 GB I already runÂ Â it so it&#39;s going to sh start but you&#39;re going toÂ \nsee that there&#39;s a little bit of a a boot up aÂ Â boot up time before we get the model to run thisÂ \nis a small model gbt2 really medium size so hereÂ Â we have uh a prompt uh uh you doing and we shouldÂ \nget some response we have warning here I&#39;m doingÂ Â well how about you so this is basically the modelÂ \nis running using python uh tensor flow and theÂ Â Transformer library from um from high in face asÂ \nyou can see to use this you really need to codeÂ Â the behavior because Hugging Face only providesÂ \nthe the the the part um that deals with the modelÂ Â how to start how to encode and decode the dataÂ \nbut maybe of the model of the of the applicationÂ Â you need you need to code it you need to developÂ \nit in your application so regarding hanging faceÂ Â Transformers we can find the pros is that uh itÂ \nhandles the model downloaded automatically andÂ Â we have Snippets in the hanging face side toÂ \nto use the models it&#39;s the best thing we haveÂ Â for experimenting and learning about machineÂ \nlearning and of course you can integrate theÂ Â code into your own product for the cons H you doÂ \nneed a solid understanding of machine learningÂ Â and natural language processing you know thisÂ \nis something you will need to learn uh to useÂ Â this libraries uh efficiently and you need to codeÂ \nthe application the behavior the logic everythingÂ Â you know it&#39;s uh on your on your side to to doÂ \nH do you you need also to know how models areÂ Â configured for best performance and it&#39;s not asÂ \nfast as other alternatives we&#39;ll see a little bitÂ Â later so you really need a powerful machine to runÂ \nthem locally so Lang chain uh is a framework forÂ Â building language applications on top of yourÂ \nmodels uh it&#39;s an ecosystem that comprises uhÂ Â connections to models they can be local or remoteÂ \nuh and all kinds of middleware to to augment yourÂ Â application it supports uh Vector databasesÂ \nand all kinds of of uh templating to create uhÂ Â an application with python that uses a languageÂ \nmodel as an engine if we go for example here toÂ Â components we will find all the large languageÂ \nmodels that are supported by Lang chain we canÂ Â even use a um model from H haging face we have uhÂ \nIntegrations with Lama and kind of tools we canÂ Â even integrate with open AI with this not whatÂ \nwe want because we want something that runs onÂ Â our Hardware so if you want to run the model withÂ \nLang chain you will still need the TransformersÂ Â library and by torch or tensor flow depending onÂ \nthe model you also will need to install uh HuggingÂ Â Face Hub you know this is something a tool that&#39;sÂ \nuseful to for for managing models and of courseÂ Â we will have to install Lang chain itself so nowÂ \nlet&#39;s create as the same version we we did withÂ Â Transformers but this times we&#39;re going to useÂ \nLang chain uh chat and we are going to paste theÂ Â code here we are using the high in face pipelineÂ \nthis is a part of the L chain project uh and weÂ Â we are using the the pipeline from High faceÂ \nthe same model of before uh dialog GPT mediumÂ Â and then we have the prompts this is all in theÂ \ndocumentation but basically uh we are using whatÂ Â you already have for from hang face TransformersÂ \nand we&#39;re supplying here a question so let&#39;s seeÂ Â if it works and should this is not a chat it justÂ \ngoing to reply to my question which is what isÂ Â encephalography so after about 10 or 20 secondsÂ \nwe have the answer uh and it&#39;s working so we canÂ Â use this uh to build uh our chat application soÂ \nthe so LangChain successfully accessed the modelÂ Â from Hugging Face and that uh means it&#39;s workingÂ \nand with this is the start obviously we can use aÂ Â lot of more tools um that Lang chain you providesÂ \nto build a very complex application with lots ofÂ Â features but this is uh the basis for running aÂ \nmodel on L chain so going over LangChain we canÂ Â sum up the cons and the pros in one list uh theÂ \npros is that is easy to use because it has anÂ Â ecosystem you can run local and remote mode sideÂ \nto side and while you need to be a developer youÂ Â need you don&#39;t need to be a machine learningÂ \nspecialist like with Transformers so you canÂ Â focus on the logic of the application but youÂ \nstill need to develop the application uh theÂ Â cons is the same as Transformers because uh it&#39;sÂ \nas slow compared to other Alternatives we&#39;ll seeÂ Â later it&#39;s still running on python so you stillÂ \nneed a good machine to run a model locally soÂ Â so the third uh way of running a local llm isÂ \nwith Llama CPP this uh it&#39;s support of LlamaÂ Â models from Facebooks and meta that run on CNÂ \nC++ and they we are going to see that thereÂ Â are most performant they they run bigger modelsÂ \non smaller Hardware I&#39;m able to run uh a biggerÂ Â model in this laptop something that I couldn&#39;t doÂ \nwith Hugging Face for example or lunching so hereÂ Â we have a basically an a Very optimized version ofÂ \na model and they they have very a good uh supportÂ Â for Apple silicon so in this is ideal at leastÂ \nfor me and for everyone that&#39;s using uh M1 onÂ Â M2 chips um so it&#39;s a good way of of running aÂ \na local llm H using less resources okay to useÂ Â jam. CPP we are going to need to basically cloneÂ \nthe repository we&#39;re going to clone the repositoryÂ Â once clone we can build the project with makeÂ \nso now we can execute Main and we see that it&#39;sÂ Â not running because we don&#39;t have a model uhÂ \nthe Lama CPP uses a special format that&#39;s uhÂ Â designed for this project that&#39;s gguf this is oneÂ \nway of storing uh the model format uh it&#39;s uh oneÂ Â of the most modern ways of storing uh a model uhÂ \ncompared with for example pytorch or tensorflowÂ Â that uses bin or H5 uh this uh um format is uh isÂ \nbetter you know so we are going to go to High INFÂ Â fate and locate one of the models in this formatÂ \ndownload it and then test Lama CPP so back toÂ Â Hugging Face let&#39;s pick a Lama 2 file there&#39;sÂ \na few if you want the official Lama files uhÂ Â from meta you do you need to create an account onÂ \nHugging Face and the email must match the one youÂ Â have in Facebook and you need to request accessÂ \nyou know and that takes a couple of days uh butÂ Â uh if we don&#39;t want to do that there&#39;s also a fewÂ \nmodels out there uploaded by individuals um we canÂ Â pick for example um this one Lama uh Lama 2 7Â \nbillion parameters and it&#39;s in GGUF uh you canÂ Â see here in file versions the uh model files thereÂ \nare different uh types of models the Q comes fromÂ Â quantization so uh it depends on how many bits theÂ \nmodel uses H we can download for testing the lateÂ Â the smallest quantization model and it will takeÂ \na while but once we have that we can execute itÂ Â on our Llama.CPP okay so now now that I have theÂ \nmodel downloaded I can uh use uh DM and call thatÂ Â I have it in my temp file and it&#39;s going to startÂ \nand since I didn&#39;t produce I didn&#39;t ask anything IÂ Â need put any prompt it just start spotting randomÂ \ntext so let me cancel that okay and let&#39;s startÂ Â again and now we&#39;re going to use um uh the -pÂ \nparameter and this is going to uh initializeÂ Â with some prompt and we should hopefully startÂ \ngetting some information uh so there&#39;s a lotÂ Â of more options here we can create a templateÂ \nwhich would be the best you know to interact withÂ Â Llama CPP but it seems that it&#39;s generatingÂ \nsome some output so let&#39;s let&#39;s see what weget so we got an answer here and for referenceÂ \nthis is a model that wouldn&#39;t run with a p orÂ Â tensor flow so Lamma CPP is really moreÂ \nperformant month we get at the end someÂ Â uh Benchmark about uh time you know and uh soÂ \nit&#39;s very interesting and it allows me to runÂ Â things that I would normally be able to do goingÂ \nover to Llama CPP the pr is that it&#39;s a lot moreÂ Â performant you see that we can run models IÂ \ncouldn&#39;t have run with python uh thanks toÂ Â being pure C and it has a lot of optimizationsÂ \nand can run bigger models with less hardwareÂ Â and I can run them in the command line or as a aÂ \nbrowser application and there&#39;s a good number ofÂ Â options to tweak the behavior of the models youÂ \nknow and if you want to build an application youÂ Â can still do that because it has bindings uh forÂ \nvarious languages so you can have uh your logicÂ Â in for example JavaScript and not GS and theÂ \nmodel actually be running with h jam. CPP forÂ Â the cons H is that it doesn&#39;t support all modelsÂ \nin Hugging Face only Lama uh models or there&#39;s aÂ Â subset of model that are supported uh you haveÂ \nto build the tool so you have you need some uhÂ Â knowledge and tools installed and it&#39;s not asÂ \nuser friendly as some tools we&#39;ll see lateron so while we&#39;re talking about Lama Cppl thisÂ \nis another project that&#39;s related from MozillaÂ Â it&#39;s called Lamafile or Llamafile and basicallyÂ \nthis is one portable script you can run anywhereÂ Â you can run it on Linux Mac OS or even windowsÂ \nand basically you can download this file andÂ Â download a model and run it without having toÂ \ncompile anything and you can even embed theÂ Â model in an executable file that runs anywhere soÂ \nyou can share your models easily and other peopleÂ Â can run them just by double clicking on the fileÂ \nwhich is great so to use this project uh we haveÂ Â a few options we can download the file directlyÂ \nand it will just work but since I already have aÂ Â model downloaded I don&#39;t want to download itÂ \nagain I can just go here and use the projectÂ Â with external weights so I will download the MacÂ \nOS version going to download the file this worksÂ Â both on Mac OS and Linux and there&#39;s a separateÂ \none one for Windows we need all so to make it letÂ Â me see executable and running it will throw anÂ \narrow because we don&#39;t have a model it&#39;s lookingÂ Â for the wrong model but we can always put the-Â \nM and Supply the model we already have from uhÂ Â the last example and it&#39;s going to load it uh inÂ \nthis case it&#39;s not a command line application itÂ Â will open a browser uh where we can uh interactÂ \nwith the model so so this is the starting pageÂ Â as you can see this is just a Llama.CPP and weÂ \nhave a few options here uh to to to tweak theÂ Â model but let&#39;s let&#39;s let&#39;s um simply talk to toÂ \nthe model and see what happens so it&#39;s startingÂ Â to produce text let&#39;s ask the same thing weÂ \nasked before and we should since the modelÂ Â is the same it should provide a very similarÂ \nanswer so here uh the model is is working uhÂ Â more or less at the same speed but in this caseÂ \nwe didn&#39;t have to build or clone in repositoryÂ Â just download one file on one model and youÂ \ncan see the the answer is shorter because theÂ Â options we we supplied uh when when we startedÂ \nuh the the model uh we also have the option toÂ Â upload images uh and ask the model about theÂ \nimages when the model is multimodal in thisÂ Â case it should work uh so this is I think anÂ \neasy alternative to running Llama.CPP you haveÂ Â the same benefits but uh with an easier install soÂ \ngoing over L file we can sum up the pros and consÂ Â in one list H the pros is the same as Llama.CPPÂ \nit&#39;s fast and you can run big models on a smallÂ Â machine and the the other Advantage is that youÂ \ncan build a single executable file you can uhÂ Â share with other developers to publish and itÂ \nwill run on any machine the cons is the same asÂ Â Llama.CPP because you don&#39;t have all the modelsÂ \navailable for you only the the models availableÂ Â for Lama CPP and there&#39;s yet another uh LlamaÂ \nversion here it&#39;s Ollama.ai. uh this site willÂ Â uh leave give you a an executable file for LinuxÂ \nor Macos and once you download this and installÂ Â it in your machine in installs a command lineÂ \ntool H and downloads the Llama file the LlamaÂ Â model sorry directly so it&#39;s I think this is theÂ \neasiest way to run Al file because you don&#39;t haveÂ Â to you just double click and then run your yourÂ \nyour query on the terminal uh let&#39;s see how itÂ Â works so with Ollama we have to download the toolÂ \nand install it and then pick one of these modelsÂ Â uh that are here for example Lama 2 we can onceÂ \nwe install it we can go to the uh to a terminalÂ Â so once we install Ollama uh we have a service onÂ \nthe taskbar and then uh a command on the commandÂ Â line and the first thing we need is to pull aÂ \nmodel you know we we saw that we have a llama2Â Â model um available so this is the first step toÂ \npull the the model and next we run Ollama runÂ Â and the model name and it should start a commandÂ \nline session we can we can talk with the modelhere let&#39;s ask the model the same thingÂ \nwe asked so far to see if we have a goodanswer so we have a pretty um we have a prettyÂ \nsimilar answer it&#39;s a little bit slower thanÂ Â Lama P or Lama file but it&#39;s really easy to getÂ \nuh the installation going and to download modelsÂ Â now with Llama I think this is the easiest toÂ \ninstall and use uh way to run a llama modelÂ Â it&#39;s just double clicking and and it installsÂ \na service on your machine and then you openÂ Â a terminal and and just run uh select your modelÂ \nand and run your your prompts and get results youÂ Â know maybe the the you can run Llama and VicuÃ±aÂ \nmodels very easily and it runs really fast H forÂ Â the cons is you do have a limited set of models HÂ \nthere&#39;s uh less models than on Llama CPP currentlyÂ Â maybe that changes in the future uh the otherÂ \nthing is that you can&#39;t manage the models andÂ Â the tool manages downloading the models so uhÂ \nif you already have models downloaded maybe youÂ Â need to download them again there&#39;s not a lot ofÂ \noptions to tweak this is really a tool dedicatedÂ Â to a user uh that isn&#39;t interested in using theÂ \nlanguage model not in tweaking the the things andÂ Â the options and there&#39;s no windows versionÂ \nso if you&#39;re on Windows this won&#39;t work foryou so the last product we are going to seeÂ \ntoday is GPT4All this is the most user friendlyÂ Â interface because it includes Ai and it allowsÂ \nyou to Simply install the application select oneÂ Â model um it automatically downloads it and you canÂ \nrun uh your queries you can even uh add your ownÂ Â documents into into the application and you wouldÂ \ngo and index them and once you have the documentsÂ Â index you can query them you can talk to yourÂ \ndocuments you can ask the model to summarize orÂ Â translate so this is a really good way of workingÂ \nwith documents locally without sending them overÂ Â the wire you know and keeping them private andÂ \nand and safe so to get started we need to downloadÂ Â the installer for our system so on we install theÂ \napplication and start it we get this window veryÂ Â similar to GPT the first thing is to go here andÂ \ngo to downloads and select one of the models hereÂ Â you will find the the available models um and weÂ \ncan select one of these and I already installedÂ Â Mistal OpenOrca or but we I could download anyÂ \nothers we have the size the sizes and we can alsoÂ Â interact with open AI with the I but let&#39;s useÂ \nthis model that I already downloaded another thingÂ Â that&#39;s uh useful here is to go to the options hereÂ \nand here we can change the prompt for the modelÂ Â temperature and all the parameters uh to tweakÂ \nit and one thing that this application has is theÂ Â that you can add a pad a folder for example I useÂ \nmy document folder here and we it will index thatÂ Â and and once it&#39;s indexed I can simply go here andÂ \nenable this folder and now the model has access toÂ Â all my uh files H in that directory for exampleÂ \nthis directory has a lot of uh books on cicd andÂ Â we can ask for example that uh the model to giveÂ \nus a definition uh depending on uh the files itÂ Â finds in in in my docs in my ebooks directory soÂ \nas you can see we have an answer here and it evenÂ Â provides the context uh you know uh it identifiesÂ \nthe book uh and summarize a definition from uh myÂ Â my document so it&#39;s really a powerful featureÂ \nof course we can use the model normally likeÂ Â any other model ask questions and everything butÂ \nwith documents uh we have uh more Alternatives soÂ Â for gbt for all the cross is that it has a PolishÂ \nguy it&#39;s a user friendl option we&#39;ve seen today uhÂ Â we can use local and remote online models likeÂ \nopen AI it supports uh uh 30 billion parameterÂ Â models easily in my machine so it&#39;s performingÂ \ngreat and you can add your own documents forÂ Â context so you can keep your data safe in yourÂ \nmachine for the cons I would say that there&#39;sÂ Â a very limited amount of models and some of theÂ \nmodels are not able to be used commercially soÂ Â the choice of how to run a large language modelÂ \nwill depend on your needs if you want chat GPT uhÂ Â you can use GPT4All which is one of the best onesÂ \naround but if you want to build a project on topÂ Â of a language model you will need something likeÂ \nLLama.CPP or even python if your goal is learningÂ Â machine learning uh you need to Learn PythonÂ \nand use the the libraries for machine learningÂ Â so I hope I made a case for running your own largeÂ \nlanguage model the models are only getting betterÂ Â and I think the gap between uh closed and openÂ \nsource models are getting shorter and smallerÂ Â all the time and especially there&#39;s a a case forÂ \nusing local uh language models because you are inÂ Â total control of your own data that&#39;s all for nowÂ \nI really do want to explore AI in more detail inÂ Â the future so if you have any request feel freeÂ \nto leave a comment if you like this video pleaseÂ Â leave us like And subscribe so you don&#39;t miss theÂ \nnext one thank you for watching and have a niceone",
    "token_count_estimate": 5655
}